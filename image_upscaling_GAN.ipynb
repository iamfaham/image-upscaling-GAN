{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamfaham/image-upscaling-GAN/blob/main/image_upscaling_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# real-ESRGAN"
      ],
      "metadata": {
        "id": "qNXMT8UsP6fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub lpips torchsummary opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df70kJqPCiJQ",
        "outputId": "88ca7019-1b43-46e4-9233-a4cf9a185eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.2.1->lpips) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# Real-ESRGAN Style Training Script (Colab Ready)\n",
        "# Datasets: DIV2K + Flickr2K via KaggleHub\n",
        "# ================================================\n",
        "import os, random, glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.models import vgg19\n",
        "from torchvision import transforms # Import transforms\n",
        "\n",
        "import lpips\n",
        "\n",
        "# ============================\n",
        "# Dataset download via KaggleHub\n",
        "# ============================\n",
        "import kagglehub\n",
        "\n",
        "def get_dataset_path(kaggle_id):\n",
        "    base_path = kagglehub.dataset_download(kaggle_id)\n",
        "    candidates = []\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        if any(f.endswith((\".png\",\".jpg\",\".jpeg\")) for f in files):\n",
        "            candidates.append(root)\n",
        "    if len(candidates) == 0:\n",
        "        raise RuntimeError(f\"No image files found under {base_path}\")\n",
        "    return candidates[0]\n",
        "\n",
        "DIV2K_DIR = get_dataset_path(\"soumikrakshit/div2k-high-resolution-images\")\n",
        "FLICKR2K_DIR = get_dataset_path(\"hliang001/flickr2k\")\n",
        "\n",
        "print(\"DIV2K sample:\", glob.glob(os.path.join(DIV2K_DIR,\"*.png\"))[:3])\n",
        "print(\"Flickr2K sample:\", glob.glob(os.path.join(FLICKR2K_DIR,\"*.png\"))[:3])\n",
        "\n",
        "# ============================\n",
        "# Realistic Degradation Function\n",
        "# ============================\n",
        "def degrade_image_pil(hr_pil):\n",
        "    hr = np.array(hr_pil).astype(np.uint8)\n",
        "    h, w = hr.shape[:2]\n",
        "\n",
        "    # Blur\n",
        "    if random.random() < 0.7:\n",
        "        k = random.choice([1,3,5,7])\n",
        "        if k > 1:\n",
        "            hr = cv2.GaussianBlur(hr, (k,k), 0)\n",
        "\n",
        "    # Downscale with random factor\n",
        "    scale_choice = random.choice([2,3,4])\n",
        "    interp_down = random.choice([cv2.INTER_AREA, cv2.INTER_LINEAR, cv2.INTER_CUBIC])\n",
        "    lr_small = cv2.resize(hr, (w//scale_choice, h//scale_choice), interpolation=interp_down)\n",
        "\n",
        "    # JPEG compression\n",
        "    if random.random() < 0.8:\n",
        "        q = random.randint(30, 95)\n",
        "        _, enc = cv2.imencode('.jpg', lr_small, [int(cv2.IMWRITE_JPEG_QUALITY), q])\n",
        "        lr_small = cv2.imdecode(enc, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Add Gaussian noise\n",
        "    if random.random() < 0.5:\n",
        "        sigma = random.uniform(0, 8)\n",
        "        noise = np.random.randn(*lr_small.shape) * sigma\n",
        "        lr_small = np.clip(lr_small + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Contrast jitter\n",
        "    if random.random() < 0.5:\n",
        "        alpha = random.uniform(0.9, 1.1)\n",
        "        lr_small = np.clip(lr_small * alpha, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(lr_small)\n",
        "\n",
        "# ============================\n",
        "# Dataset\n",
        "# ============================\n",
        "def center_crop(img: Image.Image, size: int) -> Image.Image:\n",
        "    \"\"\"Crop the image to a square of given size at the center.\"\"\"\n",
        "    w, h = img.size\n",
        "    left = (w - size) // 2\n",
        "    top = (h - size) // 2\n",
        "    right = left + size\n",
        "    bottom = top + size\n",
        "    return img.crop((left, top, right, bottom))\n",
        "\n",
        "class RealSRDataset(Dataset):\n",
        "    def __init__(self, hr_dir, scale=4, hr_patch=128, train=True, augment=True):\n",
        "        self.paths = sorted(Path(hr_dir).glob(\"*.png\")) + sorted(Path(hr_dir).glob(\"*.jpg\"))\n",
        "        if len(self.paths) == 0:\n",
        "            raise RuntimeError(f\"No images found in {hr_dir}\")\n",
        "        self.scale = scale\n",
        "        self.hr_patch = hr_patch\n",
        "        self.train = train\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "\n",
        "        # --- fixed HR crop ---\n",
        "        if self.train:\n",
        "            if w < self.hr_patch or h < self.hr_patch:\n",
        "                img = img.resize((max(w, self.hr_patch), max(h, self.hr_patch)), Image.BICUBIC)\n",
        "                w, h = img.size\n",
        "            x = random.randint(0, w - self.hr_patch)\n",
        "            y = random.randint(0, h - self.hr_patch)\n",
        "            hr = img.crop((x, y, x + self.hr_patch, y + self.hr_patch))\n",
        "        else:\n",
        "            hr = center_crop(img, self.hr_patch)\n",
        "\n",
        "        # --- generate LR patch (fixed size) ---\n",
        "        lr_size = self.hr_patch // self.scale\n",
        "        degrade = transforms.Compose([\n",
        "            transforms.Resize((lr_size, lr_size), interpolation=Image.BICUBIC),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        lr = degrade(hr)\n",
        "\n",
        "        hr = TF.to_tensor(hr)\n",
        "\n",
        "        # --- augmentation ---\n",
        "        if self.train and self.augment and random.random() < 0.5:\n",
        "            lr = torch.flip(lr, [2])\n",
        "            hr = torch.flip(hr, [2])\n",
        "        if self.train and self.augment and random.random() < 0.5:\n",
        "            lr = torch.flip(lr, [1])\n",
        "            hr = torch.flip(hr, [1])\n",
        "\n",
        "        return lr, hr\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Model: Generator & Discriminator\n",
        "# ============================\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, channels=64, growth=32):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            in_c = channels + i * growth\n",
        "            self.layers.append(nn.Conv2d(in_c, growth if i<4 else channels, 3, 1, 1))\n",
        "    def forward(self, x):\n",
        "        inputs = [x]\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out = F.leaky_relu(layer(torch.cat(inputs, 1)), 0.2) if i<4 else layer(torch.cat(inputs, 1))\n",
        "            inputs.append(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, channels=64, growth=32):\n",
        "        super().__init__()\n",
        "        self.rdb1 = ResidualDenseBlock(channels, growth)\n",
        "        self.rdb2 = ResidualDenseBlock(channels, growth)\n",
        "        self.rdb3 = ResidualDenseBlock(channels, growth)\n",
        "    def forward(self, x):\n",
        "        return self.rdb3(self.rdb2(self.rdb1(x))) * 0.2 + x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale=4, channels=64, blocks=8):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, channels, 3, 1, 1)\n",
        "        self.trunk = nn.Sequential(*[RRDB(channels) for _ in range(blocks)])\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "        up = []\n",
        "        for _ in range(int(np.log2(scale))):\n",
        "            up += [nn.Conv2d(channels, channels*4, 3, 1, 1), nn.PixelShuffle(2), nn.LeakyReLU(0.2, inplace=True)]\n",
        "        self.up = nn.Sequential(*up)\n",
        "        self.conv3 = nn.Conv2d(channels, 3, 3, 1, 1)\n",
        "    def forward(self, x):\n",
        "        fea = self.conv1(x)\n",
        "        trunk = self.conv2(self.trunk(fea))\n",
        "        fea = fea + trunk\n",
        "        fea = self.up(fea)\n",
        "        return torch.clamp(self.conv3(fea), 0, 1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_ch=3):\n",
        "        super().__init__()\n",
        "        def block(in_f, out_f, stride=1):\n",
        "            return [nn.Conv2d(in_f, out_f, 3, stride, 1),\n",
        "                    nn.BatchNorm2d(out_f),\n",
        "                    nn.LeakyReLU(0.2, inplace=True)]\n",
        "        layers = []\n",
        "        in_f = in_ch\n",
        "        for out_f in [64,128,256,512]:\n",
        "            layers += block(in_f, out_f, stride=2)\n",
        "            in_f = out_f\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ============================\n",
        "# Losses & Helpers\n",
        "# ============================\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg19(pretrained=True).features[:35].eval()\n",
        "        for p in vgg.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.vgg = vgg\n",
        "    def forward(self, x, y):\n",
        "        return F.l1_loss(self.vgg(x), self.vgg(y))\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.model = model\n",
        "        self.shadow = {k: v.clone().detach() for k,v in model.state_dict().items()}\n",
        "        self.decay = decay\n",
        "    def update(self):\n",
        "        for k, v in self.model.state_dict().items():\n",
        "            self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1-self.decay)\n",
        "    def apply_shadow(self):\n",
        "        self.backup = {k: v.clone() for k,v in self.model.state_dict().items()}\n",
        "        self.model.load_state_dict(self.shadow)\n",
        "    def restore(self):\n",
        "        self.model.load_state_dict(self.backup)\n",
        "\n",
        "# ============================\n",
        "# Training Setup\n",
        "# ============================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SCALE = 4\n",
        "HR_PATCH = 128\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_div = RealSRDataset(DIV2K_DIR, scale=SCALE, hr_patch=HR_PATCH, train=True)\n",
        "train_flickr = RealSRDataset(FLICKR2K_DIR, scale=SCALE, hr_patch=HR_PATCH, train=True)\n",
        "train_ds = ConcatDataset([train_div, train_flickr])\n",
        "val_ds   = RealSRDataset(DIV2K_DIR, scale=SCALE, hr_patch=HR_PATCH, train=False)\n",
        "\n",
        "import multiprocessing\n",
        "num_workers = min(2, max(0, multiprocessing.cpu_count() - 1))\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=(DEVICE==\"cuda\"))\n",
        "val_dl   = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=min(1,num_workers), pin_memory=(DEVICE==\"cuda\"))\n",
        "\n",
        "G = Generator(scale=SCALE).to(DEVICE)\n",
        "D = Discriminator().to(DEVICE)\n",
        "percep = PerceptualLoss().to(DEVICE)\n",
        "lpips_metric = lpips.LPIPS(net='alex').to(DEVICE)\n",
        "\n",
        "optG = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0.9,0.999))\n",
        "optD = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.9,0.999))\n",
        "\n",
        "ema = EMA(G)\n",
        "\n",
        "# ============================\n",
        "# Training Loop (shortened)\n",
        "# ============================\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "\n",
        "max_steps = 15000   # for Colab demo; increase to ~15000 for stronger training\n",
        "log_interval = 500\n",
        "save_interval = 3000\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "step = 0\n",
        "while step < max_steps:\n",
        "    for lr_imgs, hr_imgs in train_dl:\n",
        "        step += 1\n",
        "        lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
        "\n",
        "        # D step\n",
        "        optD.zero_grad()\n",
        "        sr = G(lr_imgs)\n",
        "        pred_real = D(hr_imgs)\n",
        "        pred_fake = D(sr.detach())\n",
        "        loss_D = F.mse_loss(pred_real, torch.ones_like(pred_real)) + F.mse_loss(pred_fake, torch.zeros_like(pred_fake))\n",
        "        loss_D.backward(); optD.step()\n",
        "\n",
        "        # G step\n",
        "        optG.zero_grad()\n",
        "        pred_fake_forG = D(sr)\n",
        "        adv_loss = F.mse_loss(pred_fake_forG, torch.ones_like(pred_fake_forG))\n",
        "        pix_loss = F.l1_loss(sr, hr_imgs)\n",
        "        perc_loss = percep(sr, hr_imgs)\n",
        "        loss_G = pix_loss + 0.01*perc_loss + 0.005*adv_loss\n",
        "        loss_G.backward(); optG.step()\n",
        "\n",
        "        ema.update()\n",
        "\n",
        "        if step % log_interval == 0:\n",
        "            print(f\"[Step {step}] D_loss: {loss_D.item():.4f} | G_loss: {loss_G.item():.4f}\")\n",
        "\n",
        "        if step % save_interval == 0:\n",
        "            torch.save(G.state_dict(), f\"checkpoints/G_step{step}.pth\")\n",
        "\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "\n",
        "# ============================\n",
        "# Validation\n",
        "# ============================\n",
        "G.eval(); ema.apply_shadow()\n",
        "psnr_list, ssim_list, lpips_list = [], [], []\n",
        "for i, (lr, hr) in enumerate(val_dl):\n",
        "    if i >= 20: break\n",
        "    lr, hr = lr.to(DEVICE), hr.to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        sr = G(lr)\n",
        "    hr_np = hr.squeeze().permute(1,2,0).cpu().numpy()\n",
        "    sr_np = sr.squeeze().permute(1,2,0).cpu().numpy()\n",
        "    psnr_list.append(psnr(hr_np, sr_np, data_range=1.0))\n",
        "    ssim_list.append(ssim(hr_np, sr_np, channel_axis=2, data_range=1.0))\n",
        "    lpips_list.append(lpips_metric(sr, hr).item())\n",
        "\n",
        "print(\"Avg PSNR:\", np.mean(psnr_list))\n",
        "print(\"Avg SSIM:\", np.mean(ssim_list))\n",
        "print(\"Avg LPIPS:\", np.mean(lpips_list))\n",
        "ema.restore()\n",
        "\n",
        "# ============================\n",
        "# Inference on Your Own Image\n",
        "# ============================\n",
        "def upscale_image(img_path, model_path, scale=4, save_path=\"output.png\"):\n",
        "    model = Generator(scale=scale).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    lr_t = TF.to_tensor(img).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        sr = model(lr_t)\n",
        "    sr_img = sr.squeeze().permute(1,2,0).cpu().numpy()\n",
        "    sr_img = (sr_img*255).astype(np.uint8)\n",
        "    Image.fromarray(sr_img).save(save_path)\n",
        "    print(f\"Saved upscaled image to {save_path}\")\n",
        "\n",
        "# Example usage:\n",
        "upscale_image(\"input.jpg\", \"checkpoints/G_step15000.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-KVPM2zP5op",
        "outputId": "c8c9ac54-ce9b-4fcd-efde-0377ef575cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/soumikrakshit/div2k-high-resolution-images?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.71G/3.71G [02:59<00:00, 22.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/hliang001/flickr2k?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.0G/20.0G [15:49<00:00, 22.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIV2K sample: ['/root/.cache/kagglehub/datasets/soumikrakshit/div2k-high-resolution-images/versions/1/DIV2K_train_HR/DIV2K_train_HR/0137.png', '/root/.cache/kagglehub/datasets/soumikrakshit/div2k-high-resolution-images/versions/1/DIV2K_train_HR/DIV2K_train_HR/0719.png', '/root/.cache/kagglehub/datasets/soumikrakshit/div2k-high-resolution-images/versions/1/DIV2K_train_HR/DIV2K_train_HR/0059.png']\n",
            "Flickr2K sample: ['/root/.cache/kagglehub/datasets/hliang001/flickr2k/versions/1/Flickr2K/Flickr2K_LR_unknown/X3/001223x3.png', '/root/.cache/kagglehub/datasets/hliang001/flickr2k/versions/1/Flickr2K/Flickr2K_LR_unknown/X3/000654x3.png', '/root/.cache/kagglehub/datasets/hliang001/flickr2k/versions/1/Flickr2K/Flickr2K_LR_unknown/X3/001872x3.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:07<00:00, 74.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 143MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "[Step 500] D_loss: 0.0799 | G_loss: 0.0467\n",
            "[Step 1000] D_loss: 0.0927 | G_loss: 0.0591\n",
            "[Step 1500] D_loss: 0.1592 | G_loss: 0.0433\n",
            "[Step 2000] D_loss: 0.0584 | G_loss: 0.0445\n",
            "[Step 2500] D_loss: 0.0117 | G_loss: 0.0482\n",
            "[Step 3000] D_loss: 0.0096 | G_loss: 0.0436\n",
            "[Step 3500] D_loss: 0.0022 | G_loss: 0.0427\n",
            "[Step 4000] D_loss: 0.0022 | G_loss: 0.0385\n",
            "[Step 4500] D_loss: 0.0030 | G_loss: 0.0371\n",
            "[Step 5000] D_loss: 0.0105 | G_loss: 0.0487\n",
            "[Step 5500] D_loss: 0.3187 | G_loss: 0.0391\n",
            "[Step 6000] D_loss: 0.0402 | G_loss: 0.0476\n",
            "[Step 6500] D_loss: 0.0421 | G_loss: 0.0417\n",
            "[Step 7000] D_loss: 0.0527 | G_loss: 0.0466\n",
            "[Step 7500] D_loss: 0.2620 | G_loss: 0.0420\n",
            "[Step 8000] D_loss: 0.5213 | G_loss: 0.0430\n",
            "[Step 8500] D_loss: 0.4668 | G_loss: 0.0382\n",
            "[Step 9000] D_loss: 0.2695 | G_loss: 0.0297\n",
            "[Step 9500] D_loss: 0.0999 | G_loss: 0.0471\n",
            "[Step 10000] D_loss: 0.0124 | G_loss: 0.0557\n",
            "[Step 10500] D_loss: 0.1249 | G_loss: 0.0611\n",
            "[Step 11000] D_loss: 0.5232 | G_loss: 0.0488\n",
            "[Step 11500] D_loss: 0.0923 | G_loss: 0.0376\n",
            "[Step 12000] D_loss: 0.2561 | G_loss: 0.0395\n",
            "[Step 12500] D_loss: 0.1837 | G_loss: 0.0405\n",
            "[Step 13000] D_loss: 0.1495 | G_loss: 0.0417\n",
            "[Step 13500] D_loss: 0.6379 | G_loss: 0.0358\n",
            "[Step 14000] D_loss: 0.1489 | G_loss: 0.0390\n",
            "[Step 14500] D_loss: 0.0678 | G_loss: 0.0471\n",
            "[Step 15000] D_loss: 1.0471 | G_loss: 0.0313\n",
            "Avg PSNR: 27.012031521411437\n",
            "Avg SSIM: 0.7277844\n",
            "Avg LPIPS: 0.1982479694648646\n",
            "Saved upscaled image to output.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SAVE FILES NEEDED FOR INFERENCE ===\n",
        "import torch, textwrap\n",
        "from google.colab import files\n",
        "\n",
        "# 1) Save EMA generator weights\n",
        "ema.apply_shadow()  # swap EMA params into G\n",
        "torch.save(G.state_dict(), \"generator_sr_ema.pth\")\n",
        "ema.restore()\n",
        "\n",
        "# 2) Save inference script (architecture + usage)\n",
        "inference_code = textwrap.dedent(\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# === Generator definition (RRDBNet, same as training) ===\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, channels=64, growth=32):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            in_c = channels + i * growth\n",
        "            out_c = growth if i < 4 else channels\n",
        "            self.layers.append(nn.Conv2d(in_c, out_c, 3, 1, 1))\n",
        "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
        "    def forward(self, x):\n",
        "        inputs = [x]\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out = layer(torch.cat(inputs, 1))\n",
        "            if i < 4:\n",
        "                out = self.lrelu(out)\n",
        "            inputs.append(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, channels=64, growth=32):\n",
        "        super().__init__()\n",
        "        self.rdb1 = ResidualDenseBlock(channels, growth)\n",
        "        self.rdb2 = ResidualDenseBlock(channels, growth)\n",
        "        self.rdb3 = ResidualDenseBlock(channels, growth)\n",
        "    def forward(self, x):\n",
        "        out = self.rdb1(x)\n",
        "        out = self.rdb2(out)\n",
        "        out = self.rdb3(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class RRDBNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, nf=64, nb=8, gc=32, scale=4):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.conv_first = nn.Conv2d(in_ch, nf, 3, 1, 1)\n",
        "        self.trunk = nn.Sequential(*[RRDB(nf, gc) for _ in range(nb)])\n",
        "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1)\n",
        "        up_layers = []\n",
        "        for _ in range(int(math.log2(self.scale))):\n",
        "            up_layers += [\n",
        "                nn.Conv2d(nf, nf*4, 3, 1, 1),\n",
        "                nn.PixelShuffle(2),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            ]\n",
        "        self.upsampler = nn.Sequential(*up_layers)\n",
        "        self.conv_last = nn.Conv2d(nf, out_ch, 3, 1, 1)\n",
        "    def forward(self, x):\n",
        "        fea = self.conv_first(x)\n",
        "        trunk = self.trunk_conv(self.trunk(fea))\n",
        "        fea = fea + trunk\n",
        "        fea = self.upsampler(fea)\n",
        "        out = self.conv_last(fea)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "# === Inference helper ===\n",
        "def upscale_image(input_path, output_path=\"sr_out.png\", model_path=\"generator_sr_ema.pth\", scale=4, nb=8):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    G = RRDBNet(nb=nb, scale=scale).to(device)\n",
        "    G.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    G.eval()\n",
        "\n",
        "    img = Image.open(input_path).convert(\"RGB\")\n",
        "    t = TF.to_tensor(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sr = G(t).clamp(0,1)\n",
        "\n",
        "    sr_img = TF.to_pil_image(sr.squeeze().cpu())\n",
        "    sr_img.save(output_path)\n",
        "    print(f\"Saved: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Example usage:\n",
        "# upscale_image(\"myphoto.png\", \"myphoto_sr.png\", model_path=\"generator_sr_ema.pth\")\n",
        "\"\"\")\n",
        "\n",
        "with open(\"inference.py\", \"w\") as f:\n",
        "    f.write(inference_code)\n",
        "\n",
        "# 3) Download both files to local system\n",
        "files.download(\"generator_sr_ema.pth\")\n",
        "files.download(\"inference.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0icxBF4OKE07",
        "outputId": "51b06e98-0777-4226-9f71-7d2143873194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_44500cbd-dd6f-4b53-a075-22e8b3d8f1b2\", \"generator_sr_ema.pth\", 24455819)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_65a2d935-974c-4fdc-a4a1-f94e71b531c8\", \"inference.py\", 2871)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}